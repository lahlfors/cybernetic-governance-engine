# Sovereign Stack Configuration
# Rename this file to ".env"

# --- MODEL CONFIGURATION (Sovereign / Open Weights) ---

# Fast Path (Control Plane): Routing, JSON Formatting, Simple Execution
# Recommended: meta-llama/Llama-3.1-8B-Instruct (Open Weights)
MODEL_FAST=meta-llama/Llama-3.1-8B-Instruct
# Endpoint for Fast Model (vLLM)
VLLM_FAST_API_BASE=http://localhost:8000/v1

# Reasoning Path (Reasoning Plane): Risk Analysis, Strategy, Evaluation
# Recommended: meta-llama/Llama-3.1-70B-Instruct (Open Weights)
MODEL_REASONING=meta-llama/Llama-3.1-70B-Instruct
# Endpoint for Reasoning Model (vLLM)
# Note: For Open Weights, this should point to a separate vLLM instance hosting the large model (e.g. port 8001)
VLLM_REASONING_API_BASE=http://localhost:8001/v1

# Consensus Path: High-stakes validation (defaults to MODEL_REASONING)
MODEL_CONSENSUS=meta-llama/Llama-3.1-70B-Instruct

# --- INFRASTRUCTURE CONFIGURATION ---
GOOGLE_CLOUD_PROJECT=sovereign-stack
GOOGLE_CLOUD_LOCATION=local
# GOOGLE_GENAI_USE_VERTEXAI=0 # Disabled for Sovereign Stack

# --- GOVERNANCE & POLICY ---
# Policy Registry (Offline Pipeline)
POLICY_REGISTRY_BUCKET=local-policy-bucket
STAMP_CONFIG_BLOB=current_stamp_spec.yaml
OUTPUT_ARTIFACT_URI=gs://local/artifacts/latest_evidence.json

# Policy Engine (OPA)
OPA_URL=http://localhost:8181/v1/data/finance/allow

# --- HYBRID INFERENCE (Legacy/Shared Config) ---
VLLM_BASE_URL=http://localhost:8000/v1
VLLM_API_KEY=EMPTY

# --- OBSERVABILITY ---
# Langfuse Configuration (Hot Tier / Agent Tracing)
LANGFUSE_PUBLIC_KEY=<YOUR_PUBLIC_KEY>
LANGFUSE_SECRET_KEY=<YOUR_SECRET_KEY>
LANGFUSE_HOST=http://localhost:3000

# OpenTelemetry (Cold Tier)
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318/v1/traces
OTEL_EXPORTER_OTLP_HEADERS=

# Cold Tier Storage (Parquet)
COLD_TIER_GCS_BUCKET="local-bucket"
COLD_TIER_GCS_PREFIX="cold_tier"

# --- SERVICE CONFIGURATION ---
PORT=8080
