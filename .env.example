# Fill in your GCP project info and rename this file to ".env".

# Model Configuration (Tiered)
# Fast path: Supervisor, Data Analyst, Execution Analyst
MODEL_FAST=gemini-2.5-flash-lite

# Reasoning path: Risk Analyst, Verifier (safety-critical)
MODEL_REASONING=gemini-2.5-pro

# Consensus path: High-stakes validation (defaults to MODEL_REASONING)
MODEL_COLANGFUSE_HOST="https://cloud.langfuse.com"

# Tiered Observability (Cold Storage)
# If set, Parquet files are written to GCS. If unset, defaults to local disk (logs/cold_tier).
COLD_TIER_GCS_BUCKET="your-gcs-bucket-name"
COLD_TIER_GCS_PREFIX="cold_tier"

# Vertex AI Configuration
GOOGLE_GENAI_USE_VERTEXAI=1
GOOGLE_CLOUD_PROJECT=<YOUR_PROJECT_ID>
GOOGLE_CLOUD_LOCATION=<YOUR_PROJECT_LOCATION>
GOOGLE_CLOUD_ZONE=<YOUR_PROJECT_ZONE>  # Optional: For Zonal GKE clusters (e.g., us-central1-a)
GOOGLE_CLOUD_STORAGE_BUCKET=<YOUR_STORAGE_BUCKET>  # Only required for deployment on Agent Engine

# Governance & Policy Registry (Offline Pipeline)
POLICY_REGISTRY_BUCKET=<YOUR_POLICY_BUCKET_NAME>
STAMP_CONFIG_BLOB=current_stamp_spec.yaml
OUTPUT_ARTIFACT_URI=gs://<YOUR_POLICY_BUCKET_NAME>/artifacts/latest_evidence.json

# Policy Engine
OPA_URL=http://localhost:8181/v1/data/finance/allow

# Hybrid Inference (vLLM)
# Note: vLLM model is hardcoded to meta-llama/Llama-3.1-8B-Instruct
VLLM_BASE_URL=http://localhost:8000/v1
VLLM_API_KEY=EMPTY

# Service Configuration
PORT=8080
GOOGLE_APPLICATION_CREDENTIALS= # Path to service account key (optional for local dev)
