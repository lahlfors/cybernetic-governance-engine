syntax = "proto3";

package gateway;

service Gateway {
    // LLM Proxy: Bi-directional streaming for conversational agents
    // The client sends a single configuration + history message (or stream),
    // and the server streams back tokens.
    // For simplicity, we'll assume Client sends ONE message with full history,
    // and Server streams responses.
    rpc Chat (ChatRequest) returns (stream ChatResponse) {}

    // Tool Execution Proxy: Request-Response
    // The Agent asks to run a tool, the Gateway runs it (with governance)
    rpc ExecuteTool (ToolRequest) returns (ToolResponse) {}
}

message Message {
    string role = 1;
    string content = 2;
}

message ChatRequest {
    string model = 1;
    repeated Message messages = 2;
    float temperature = 3;
    string system_instruction = 4;

    // Mode: "planner" (streaming) or "verifier" (blocking/determinism)
    string mode = 5;

    // FSM Constraints (for Verifier)
    string guided_json = 6;  // JSON Schema string
    string guided_regex = 7;
    string guided_choice = 8; // JSON list string
}

message ChatResponse {
    string content = 1;
    bool is_final = 2; // If true, this chunk completes the response

    // Usage Metadata (optional, usually sent with final chunk)
    int32 input_tokens = 3;
    int32 output_tokens = 4;
}

message ToolRequest {
    string tool_name = 1;
    string params_json = 2; // JSON-serialized arguments
}

message ToolResponse {
    string output = 1;
    string error = 2; // Empty if success
    string status = 3; // SUCCESS, ERROR, BLOCKED
}
